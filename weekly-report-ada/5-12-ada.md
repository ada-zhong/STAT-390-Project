### Weekly Report 4: 5/12
_NOTE: These reports will be for my Kaggle progress only, as I am working with the group on MCMF._

#### Progress:
- Finished EDA
  - Looked at data types within dataset
  - Examined missing values, which led me to decide to scale and impute missing values 
    - I used a KNNImputer
  - Dropped meaningless columns (where there was only one unique value)
  - Analyzed correlation and multicollinearity among features and found high multicollinearity
- Per my plan from last week, I made a few submissions just to see my baseline RMSE
  - Achieved an RMSE of 9.37 --> significant room for improvement, but not terrible for an initial pass
- Was able to strike a better balance between Kaggle and MCMF work!


#### Problem:
- Tried to make a pipeline to tune my k-value for KNN imputation, but the runtime was too long (even on Google Colab)
  - May need to switch approach or abandon this plan
- Wrote some code to drop cols based on VIF, but took the runtime was also too long because I have to recalculate the VIF table every time I drop a feature
  - Will need to switch approach
- I need to wait to learn Neural Networks, AdaBoost/GradBoost, and ensemble models in 303-3 before applying those concepts here


#### Plan:
- Determine method to drop highly correlated/collinear columns from dataset
- Continue dimensionality reduction with PCA
- Make a couple more Kaggle submissions to see if reducing multicollinearity and dimensions decreases RMSE
- Try making predictions with a model with tuned hyperparameters, if time permits

Now that I have made much more progress on Kaggle with a set plan for how to reduce my RMSE, I intend to spend the next week focusing more on MCMF. 
